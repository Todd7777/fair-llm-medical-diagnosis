# Integrated Fairness Training Configuration

# Dataset configuration
dataset:
  name: "chexray"  # Options: chexray, pathology, retinal
  path: "data/chexray"
  image_size: [224, 224]
  use_augmentation: true
  include_clinical_text: true  # Whether to include clinical text data

# Model configuration
model:
  base_model: "microsoft/BiomedVLP-CXR-BERT-specialized"  # Pre-trained medical vision-language model
  image_encoder_name: "microsoft/swin-base-patch4-window7-224-in22k"  # Image encoder for multimodal approach
  text_encoder_name: "microsoft/BiomedNLP-PubMedBERT-base-uncased-abstract-fulltext"  # Text encoder for multimodal approach
  output_dim: 2  # Number of output classes (binary classification)

# Fairness configuration
fairness:
  approach: "multimodal"  # Options: fair_lora, adversarial, counterfactual, multimodal
  
  # Fair LoRA parameters (used by all approaches)
  lora_r: 16
  lora_alpha: 32
  lora_dropout: 0.1
  target_modules: ["query", "key", "value", "output.dense"]
  equity_scaling_factor: 0.8
  
  # Adversarial fairness parameters
  adv_hidden_dims: [256, 128]
  lambda_adv: 0.5
  grad_reversal_strength: 1.0
  
  # Counterfactual fairness parameters
  counterfactual_weight: 1.0
  causal_strength: 0.5
  num_counterfactuals: 3
  counterfactual_augmentation: true
  invariance_type: "both"  # Options: prediction, representation, both
  
  # Multi-modal fairness parameters
  temperature: 0.07
  projection_dim: 256
  modality_fusion: "attention"  # Options: attention, concat, gated
  contrastive_weight: 0.5
  use_clinical_text: true
  shared_projection: true

# Training configuration
training:
  num_epochs: 30
  batch_size: 32
  learning_rate: 2.0e-5
  weight_decay: 0.01
  warmup_steps: 500
  gradient_accumulation_steps: 1
  mixed_precision: true
  seed: 42
  early_stopping_patience: 5
  scheduler: "cosine"  # Options: linear, cosine, constant
  max_grad_norm: 1.0

# Evaluation configuration
evaluation:
  eval_steps: 100
  save_steps: 500
  eval_batch_size: 64
  compute_fairness_metrics: true
  fairness_metrics:
    - "demographic_parity"
    - "equalized_odds"
    - "equity_scaled_auc"
    - "subgroup_performance"
    - "worst_group_performance"

# Logging configuration
logging:
  log_dir: "logs"
  save_dir: "checkpoints"
  use_wandb: true
  wandb_project: "fair-llm-medical-diagnosis"
  wandb_run_name: "integrated-fairness-training"
  log_steps: 10
  save_total_limit: 3
